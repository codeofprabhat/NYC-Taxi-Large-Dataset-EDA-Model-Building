# -*- coding: utf-8 -*-
"""NYC-Taxi-Large Dataset-EDA-Model-Building

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MEYQXalyz5MdrhZ0WBbi9GOYz4tIKUuF?resourcekey=0-BvotWRftVxL4K1guFKXYmw

###### Pointers

* Data Manipulation : Data Cleaning - Missing values, NA values, merging datasets, concat
* Filtering and Sorting - filter rows on conditions
* Groupby and Aggregation - mean, median, mode
* Datetime Handling - strings to datetime
* Pivot Tables and Crosstabs - pivot tavbles to summarize data
* Text Data Processing
* Using NumPy - array ops

---

* Skill Area	What You Can Do With This Dataset
1. Data Cleaning	Handle missing/NA values in location, fare, or datetime columns.
2. Filtering & Sorting	Filter rides by payment type, passenger count, long vs short rides, etc.
3. GroupBy & Aggregation	Calculate mean trip distance or total fare per hour, per pickup zone, etc.
4. Datetime Handling	Convert pickup/dropoff columns to datetime objects; extract hours, weekdays, etc.
5. Pivot Tables/Crosstabs	Create tables showing trip count per day per hour, or fare amounts per vendor.
6. Text Data Processing	If you use the taxi zone lookup, you can explore zone names and clean text data.
7. Using NumPy	Perform vectorized calculations on fare amounts, trip durations, etc.
8. Data Read/Write	Read from CSVs (simulate F1/Spanner by writing to/from SQL or GSheets via API).
9. Pattern Recognition	Identify average fares by time of day, detect duplicate trips, analyze ride frequency.
10. Visualization	Plot histograms of fare amounts, heatmaps of pickup zones, correlations between features.
11. Optimization	Use chunking, Dask, or pyarrow to handle large files efficiently.
12. Gemini/GenAI (Optional)	Summarize trends, generate charts based on prompts (if you're using Gemini with Sheets/Colab).

---

### Data Extraction - Mounting drive with Colab. Uploading csv files to colab and importing using pandas
"""

from google.colab import drive
drive.mount('/content/drive')

"""## This file contains the Yellow Taxis Data for January 2015.
## Large Dataset Optimisation Techniques
* There are 4 csv datasets. We'll use one by one to
* Understand the structure
* Clean and analyze without heavy RAM usage
* Avoid early confusion from merging

Note: Once you finalize your workflow, apply it across the others and then combine them.

"""

# Imorting the dataset
import pandas as pd
df_1=pd.read_csv(r"/content/drive/MyDrive/NYC Taxi Large Dataset/yellow_tripdata_2015-01.csv")
df_1

df_1.shape

""":Details about Columns
* Vendor ID: code indicating the TPEP provider that provided the record.
* tpep_pickup_datetime:
The date and time when the meter was engaged.


* tpep_dropoff_datetime: The date and time when the meter was disengaged.


* passenger_count: The number of passengers in the vehicle. This is a driver-entered value.


* trip_distance: The elapsed trip distance in miles reported by the taximeter.


* pickup_longitude: Longitude where the meter was engaged.


* pickup_latitude: Latitude where the meter was engaged.


* RateCodeID: The final rate code in effect at the end of the trip.


* store_and_fwd_flag: This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server.


* dropoff_longitude: Longitude where the meter was disengaged.

### Data Manipulation - Cleaning, sorting, filtering, joining, aggregating - "Making data ready for analysis"

Basic Info and Structure - "Peek at the Data"

Function	Description
1. df.head(n)	Shows the first n rows (default is 5)
2. df.tail(n)	Shows the last n rows
3. df.shape	Returns a tuple with (rows, columns)
4. df.info()	Displays index, column names, data types, and non-null counts
5. df.columns	Lists all column names
6. df.index	Shows the index (row labels)
7. df.dtypes	Displays data types of each column
8. df.memory_usage()	Shows memory usage of each column
"""

df_1.columns

df_1.describe()

df_1.info()

df_1.index

df_1.dtypes

df_1.memory_usage()

"""### Summary Statistics

Function	Description
1. df.describe()	Summary stats for numeric columns (count, mean, std, min, max, quartiles)
2. df.describe(include='all')	Summary stats designed to summarize all columns, not just numeric ones — including categorical (object/string), boolean, and datetime types.
Note : Metrics for String/Object columns are -
* count - all non-null values in the column
* unique - number of unique values in the column
* top - (mode) -most frequent value
* freq - count of most frequent value
3. df.mean(), df.median(), df.std()	Basic statistical measures
4. df.nunique()	Count of unique values in each column
5. df.value_counts()	Frequency count of unique values in a Series (use with df['column'])
6. df.mode()	Most frequent value(s) in each column
"""

df_1.describe() #summary stats only for numeric columns

df_1.describe(include='all')
# summary stats designed to summarize all columns- categorical (object/string), boolean, and datetime types.
# Metrics for string/object based columns - unique, top(mode), frequency

# find mean for only numeric columns (skipping string/object columns)

## Filtering

df_1_numeric=df_1.select_dtypes(include='number') # filters out only numeric columns

# Similarly, we can find median and std (Standard deviation) for numeric columns

print('mean of numeric columns',df_1_numeric.mean()) # finds mean of numeric columns
print('median of numeric columns',df_1_numeric.median()) # finds median of numeric columns
print('standard deviation of numeric columns',df_1_numeric.std()) # finds std of numeric columns

# Unique, Nunique and Value_counts
# Unique - Shows unique values - output (numpy array) - what are the categories - mainly for one column, doesn't go with entire dataset
# nunique - counts unique values - output (integer) - how many categories - counts unique values of all columns in a dataset
# value_counts - count how often each value occurs - full frequency distribution
# mode - most frequent values (if there's a tie, it returns all modes)

print('UNIQUE- unique values in the column are',df_1['VendorID'].unique())
print('NUNIQUE- number of unique values are',df_1['VendorID'].nunique())
print('VALUE_COUNTS - number of times each unique values occurs are',df_1['VendorID'].value_counts())
print('MODE - most frequent value in the column is',df_1['VendorID'].mode())
print('-----------------------------------------------------------')
# print('unique_values in the column - VendorID are',df_1['VendorID'].unique(), 'and count of unique values are', df_1['VendorID'].nunique())

(df_1['VendorID'].value_counts())

df_1['VendorID'].mode()

"""### Data Quality and Nulls
Function	Description
1. df.isnull().sum()	Count of missing values in each column
2. df.notnull().sum()	Count of non-null values
3. df.duplicated().sum()	Number of duplicated rows
4. df.duplicated().value_counts()	How many rows are duplicates vs unique
5. df.isna().any()	Check if any NA values exist per column

* Note: isnull() and isna() gives the same output
"""

df_1.isnull().sum()

df_1.notnull().sum()

df_1.duplicated().sum()

"""###### No duplicate values"""

df_1.duplicated()

df_1.duplicated().value_counts()

# df_1.duplicated() - returns a boolean Series (True if a row is a duplicate of a previous one, False if it's the first occurrence).
# True if duplicate and False if it's unique
# for example
import pandas as pd

df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],
    'age': [25, 30, 25, 35, 30]
})

df

df.duplicated()

"""* 0    False  # First 'Alice',25 — unique
* 1    False  # First 'Bob',30 — unique
* 2     True  # Duplicate of index 0
* 3    False  # Unique row
* 4     True  # Duplicate of index 1


dtype: bool

"""

df.duplicated().value_counts()

"""* False    3   # Unique rows
* True     2   # Duplicated rows
* True means duplicate

#### Data Types and Conversion
* df.select_dtypes(include='number')	Select numeric columns
* df.select_dtypes(include='object')	Select object (string) columns
* df.astype('type')	                  Convert a column to a specific data type
"""

df_1.dtypes

df_numeric=df_1.select_dtypes(include='number').head()
df_numeric

df_object=df_1.select_dtypes(include='object').head()
df_object

# astype converts datatype of a column
# Here, vendorID is numeric datatype column, we changed it to string/object datatype
df_1['VendorID'].astype('object')

"""#### Data Sampling and Exploration
* df.sample(n)	Random sample of n rows
* df.corr()	Correlation between numeric features
* df.cov()	Covariance matrix
* df.unique()	Unique values in a Series (df['column'].unique())
* df['column'].value_counts(normalize=True)	Proportions of unique values
"""

df_1.sample(5) # random sample of 5 rows

# df.corr() - Correlation Matrix - Pairwise correlation between numeric columns in a dataframe
# Correlation coefficinet ranges from -1 to 1
# 1: Perfect correlation
# 0: No correlation
# -1: Perfect negative correlation
# --------------------------------------
# NOTE: correlation coefficient close to 1 or -1 = strong relationship
# --------------------------------------
# Methods Available:
# 'pearson' (default): Measures linear correlation.
# 'kendall': Measures ordinal association.
# 'spearman': Measures rank correlation.​

df_numeric=df_1.select_dtypes(include='number').head()  ### only the numeric columns
df_numeric.corr() # correlation between numeric features

# df.cov() — Covariance Matrix - Calculates the pairwise covariance between numeric columns in a DataFrame
# Covariance: Indicates the direction of the linear relationship between variables
# Positive covariance: Variables tend to increase together
# Negative covariance: One variable tends to increase when the other decrease

df_cov=df_numeric.cov() # covariance matrix
df_cov

df_1['VendorID'].value_counts(normalize=True)

# ​df['column'].value_counts(normalize=True)
# pandas function calculates the proportions (relative frequencies) of unique values in a Series
# understanding the distribution of categorical data
# it outputs - index and values

df = pd.DataFrame({
    'Fruit': ['Apple', 'Banana', 'Apple', 'Orange', 'Banana', 'Apple']
})

# Applying value_counts(normalize=True) to the 'Fruit' column:

df['Fruit'].value_counts(normalize=True)

"""This indicates that:​

* 'Apple' constitutes 50% of the entries.

* 'Banana' constitutes approximately 33.33%.

* 'Orange' constitutes approximately 16.67%.
"""

df['Fruit'].value_counts(normalize=True)*100 #displaying it in percentages

"""Use Cases
* Data Exploration: Understand the distribution of categorical variables.

* Data Cleaning: Identify and handle imbalanced classes.

* Feature Engineering: Create features based on the frequency of categories.

## df.apply(): Applying Functions to Rows or Columns
"""

# syntax : df.apply(func, axis=0, raw=False, result_type=None, args=(), **kwds)
# func : function to apply to each column or row
# 0 apply function to each column
# 1 apply function to each row

import pandas as pd
import numpy as np

df_app = pd.DataFrame({'A': [1, 2], 'B': [10, 20]})
df_app

# Square each element in the DataFrame

df_squared=df_app.apply(np.square)
df_squared

df_app

# Sum of elements in each row
# numpy (np) - numerical calculation using python
# function applies "row" vise - horizontally -|-|-|-|-> (like this)
# Row 0: 1+10=11 | Row 1: 2+20=22
row_sums = df_app.apply(np.sum, axis=1)
row_sums

df_app

# function applies "column" vise
# ----
# |
# ----
# |
# (like this) - vertically

col_sums = df_app.apply(np.sum, axis=0)
col_sums

# row 0: 1+2 = 3
# row 1: 10+20 = 30

df_app

# Applying a Custom Function with lambda

# Add 5 to each element
df_plus_five = df_app.apply(lambda x: x + 5)
df_plus_five

"""### df.groupby().agg(): Grouped Analysis"""

# The groupby() function is used to split the data into groups based on some criteria
# agg() is used to apply aggregation functions to these groups.
# df.groupby('column_name').agg({'col1': 'func1', 'col2': 'func2', ...})
# 'column_name': The column to group by.
# 'col1', 'col2', ...: Columns to aggregate.
# 'func1', 'func2', ...: Aggregation functions like 'sum', 'mean', 'count', etc

# Sample DataFrame
df_grp = pd.DataFrame({
    'Department': ['Sales', 'Sales', 'HR', 'HR'],
    'Salary': [50000, 60000, 45000, 52000],
    'Bonus': [5000, 6000, 4000, 4200]
})

df_grp

df_grp.groupby('Department').agg({'Salary': 'mean'})

# Group by Department and calculate total Salary and total Bonus
# 'SUM' Function
df_grp.groupby('Department').agg({'Salary': 'sum', 'Bonus': 'sum'})

# Group by Department and calculate mean Salary and total Bonus
# 'MEAN' or 'AVERAGE' Function
df_grp.groupby('Department').agg({'Salary': 'mean', 'Bonus': 'sum'}) # Average Salary

df_grp

"""## pass a dictionary with multiple functions: More complex aggregation

"""

df_grp.groupby('Department').agg({'Salary': ['mean', 'max','min'], 'Bonus': ['sum', 'min','max']})

df_1.head()

# Filtering Groups Based on Conditions
# Filter groups where the average fare amount is greater than $50

filtered_groups = df_1.groupby('payment_type').filter(lambda x: x['fare_amount'].mean() > 5)
filtered_groups.head(3)

"""## **Clean The Data** - Data Wrangling, Handling Missing Values"""

df_1.head()

# check for missing values
df_1.isnull().sum()

# Displays only those columns that have at least one missing value
df_missing=df_1.isnull().sum()
print(df_missing[df_missing>0])

"""**Strategies for Handling Missing Data**

### Imputation Techniques - Replacing or Imputing missing values
"""

df_1.head()

# for numerical columns
# print(df_1['fare_amount'].isnull().sum())
# print(df_1['fare_amount'].dtype)
# df_1['fare_amount'].fillna(df_1['fare_amount'].median(), inplace=True)
df_1['fare_amount'] = df_1['fare_amount'].fillna(df_1['fare_amount'].median())
print(df_1['fare_amount'])

df_1['payment_type'].value_counts()

# for categorical columns
df_1['payment_type']=df_1['payment_type'].fillna(df_1['payment_type'].mode()[0]) # 0 is the first index number of most frequent value in the column
print(df_1['payment_type'])

"""* The mode() function in pandas returns the most frequently occurring value(s) in a Series.
* Unlike mean() or median(), which return a single value, mode() can return multiple values if there is a tie for the highest frequency.
* The result is a Series containing all modes in ascending order.
* Purpose of [0] - Using [0] selects the first mode from the Series returned by mode().

### Large Dataset + Small proportion of missing data = remove those enteries
* If greater proportion of missing values = this option might lose information, so use it, accordingly
"""

df_cleaned=df_1.dropna()
df_cleaned.isnull().sum()

"""## Date-Time Handling
* Handling datetime columns in a dataset is crucial for time-based analyses
* especially with datasets like the NYC Taxi dataset that include timestamps for pickups and drop-offs.
"""

df_1.head()

"""* pd.to_datetime() converts an argument—such as a string, integer, float, list, tuple, 1-D array, Series, or DataFrame/dict-like object—into a pandas datetime object.
* This conversion is essential for performing datetime operations like filtering by date ranges, extracting specific components (e.g., year, month), and resampling time series data.

"""

print(df_1['tpep_pickup_datetime'])
print(df_1['tpep_dropoff_datetime'])

# converts this to Pandas datetime datatype

df_1['tpep_pickup_datetime'] = pd.to_datetime(df_1['tpep_pickup_datetime'])
df_1['tpep_dropoff_datetime'] = pd.to_datetime(df_1['tpep_dropoff_datetime'])

df_1['tpep_pickup_datetime'].head()

"""* YYYY-MM-DD - Date
* HH:MM:SS - Time
"""

# After conversion, you can extract various components

# df_1['pickup_date'] = df_1['tpep_pickup_datetime'].dt.date
# df_1['pickup_date'].head(3)
# df_1['pickup_time'] = df_1['tpep_pickup_datetime'].dt.time
# df_1['pickup_time'].head(3)
# df_1['pickup_hour'] = df_1['tpep_pickup_datetime'].dt.hour
# df_1['pickup_hour'].head()
# df_1['pickup_day'] = df_1['tpep_pickup_datetime'].dt.day
# df_1['pickup_day'].head()
# df_1['pickup_weekday'] = df_1['tpep_pickup_datetime'].dt.day_name()
# df_1['pickup_weekday'].head(3)
# df_1['pickup_month'] = df_1['tpep_pickup_datetime'].dt.month
# df_1['pickup_month'].head(3)
# df_1['pickup_year'] = df_1['tpep_pickup_datetime'].dt.year
# df_1['pickup_year'].head(3)


## ---------------------------------------------------------------

print('DATE',df_1['tpep_pickup_datetime'].dt.date)
print('TIME',df_1['tpep_pickup_datetime'].dt.time)
print('HOUR',df_1['tpep_pickup_datetime'].dt.hour)
print('DAY',df_1['tpep_pickup_datetime'].dt.day)
print('YEAR',df_1['tpep_pickup_datetime'].dt.year)

## These components are useful for temporal analyses, such as identifying peak hours or seasonal trends.

"""* Trip Duration"""

df_1['trip_duration']=df_1['tpep_dropoff_datetime']-df_1['tpep_pickup_datetime']
df_1['trip_duration_minutes']=df_1['trip_duration'].dt.total_seconds()/60
df_1['trip_duration_minutes'].head()      # Trip duration in minutes

"""#### Filtering & Sorting

1. Filtering - Select rows with specific conditions
"""

# Filter by a single condition
filtered_df = df_1[df_1['trip_duration_minutes'] > 60]   # filter rows with trip_duration_time greater than 60 mins
filtered_df.head()

# rows/enteries with fare_amount>50
# syntax : new_variable=dataframe[dataframe['col] <> condition]

df_fare=df_1[df_1['fare_amount']>50]
df_fare.head()

# Filter by Multiple Conditions

filtered_df = df_1[(df_1['trip_duration_minutes'] > 60) & (df_1['fare_amount'] > 50) & (df_1['payment_type']==2)] # payment_type=2 (Cash)
filtered_df.head()

"""# # Filter Using .query() Method - A Great and readable way to filter data

"""

df_filtered = df_1.query("fare_amount > 50 and payment_type == 2 and tolls_amount > 3 and total_amount > 50 and trip_duration_minutes > 60")
df_filtered.head(6)

"""## Sorting - Sorting helps in organizing data based on column values."""

# Sort by a Single Column - To sort trips by fare amount in descending order - (Greater to Smaller)
# Syntax : df.sort_values(by = 'col_needs_to_be_sorted,ascending=True/False)

df_sorted = df_1.sort_values(by='fare_amount', ascending=False)
df_sorted.head(10)

sorted=df_1['fare_amount'].sort_values(ascending=False)
print(sorted)

# Sort by Multiple Columns

df_sorted_multiple= df_1.sort_values(by=['total_amount', 'fare_amount'], ascending=[True, False])
df_sorted_multiple

# ascending=[True, False] - This sorts the DataFrame by payment_type in ascending order and then by fare_amount in descending order.

"""### Combining Filtering and Sorting"""

top_cash_trips = df_1[df_1['payment_type'] == 2].sort_values(by='fare_amount', ascending=False).head(5)
top_cash_trips

"""# Pivot Tables & Crosstabs
# Pivot Tables - Numerical columns | aggregation (summary) | 3 Variables
* A pivot table allows you to reorganize and summarize selected columns and
* rows of data in a DataFrame to obtain a desired report.
* It's especially useful for aggregating numerical data.

## pd.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None)

* data: The DataFrame to operate on
* values: Column(s) to aggregate
* index: Column(s) to set as index (rows)
* columns: Column(s) to set as columns
* aggfunc: Aggregation function (e.g., 'mean', 'sum', 'count')
* fill_value: Value to replace missing values

## Example
### Suppose we want to find the "average fare amount" for each combination of "passenger count" and "payment type":
"""

pivot = pd.pivot_table(df_1, values='fare_amount', index='passenger_count', columns='payment_type', aggfunc='mean', fill_value=0)
pivot

"""# CrossTabs - Categorical columns | frequency of occurrences

* special case of a pivot table that calculates the frequency (count) of occurrences between two (or more) categorical variables.
* It's particularly useful for understanding the relationship between these variables

# pd.crosstab(index, columns, values=None, aggfunc=None, margins=False, normalize=False)

* index: Array-like, values to group by in the rows.
* columns: Array-like, values to group by in the columns.
* values: Array of values to aggregate according to the factors. Requires aggfunc to be specified.
* aggfunc: Function to aggregate the data.
* margins: Add row/column margins (subtotals).
* normalize: Normalize by dividing all values by the sum of values.
"""

crosstab = pd.crosstab(df_1['passenger_count'], df_1['payment_type'])
crosstab

"""# Text Data Processing"""

df_str= pd.DataFrame({'text': ['  Hello World!  ', 'Pandas is GREAT', 'Data Science ']})
df_str

df_str['text']=df_str['text'].str.lower()
df_str

df_str['text']=df_str['text'].str.strip()
df_str

# Replace specific substrings
df_str['text'] = df_str['text'].str.replace('great', 'awesome', case=False)
df_str

"""* These operations help standardize text data, making it more consistent for analysis.​

# Numpy

## 1. Array Creation
"""

import numpy as np

# Create arrays
a = np.array([1, 2, 3])             # From a list
b = np.zeros((2, 3))                # 2x3 array of zeros
c = np.ones((3, 2))                 # 3x2 array of ones
d = np.eye(3)                       # 3x3 identity matrix
e = np.arange(0, 10, 2)             # Array from 0 to 8 with step 2
f = np.linspace(0, 1, 5)            # 5 values from 0 to 1

print(a)
print(b)
print(c)
print(d)
print(e)
print(f)

"""### 2. Arithmetic Operations"""

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# Element-wise operations
sum_ab = a + b                      # [5 7 9]
diff_ab = a - b                     # [-3 -3 -3]
prod_ab = a * b                     # [4 10 18]
div_ab = a / b                      # [0.25 0.4 0.5]
exp_a = a ** 2                      # [1 4 9]

"""## 3. Statistical Operations"""

data = np.array([1, 2, 3, 4, 5])

mean = data.mean()                  # 3.0
median = np.median(data)            # 3.0
std_dev = data.std()                # 1.4142...
sum_data = data.sum()               # 15
max_val = data.max()                # 5
min_val = data.min()                # 1

"""## 4. Array Manipulation"""

# Reshape, flatten, and transpose arrays

matrix = np.array([[1, 2], [3, 4], [5, 6]])

reshaped = matrix.reshape((2, 3))   # Reshape to 2x3
flattened = matrix.flatten()        # Flatten to 1D array
transposed = matrix.T               # Transpose of the matrix

print(reshaped)
print(flattened)
print(transposed)

"""## 5. Indexing and Slicing"""

arr = np.array([[10, 20, 30], [40, 50, 60]])

element = arr[1, 2]                 # 60
row = arr[0, :]                     # [10 20 30]
column = arr[:, 1]                  # [20 50]
subarray = arr[0:2, 1:3]            # [[20 30], [50 60]]

print(arr)
print(element)
print(column)
print(subarray)

"""# EDA"""

df_1.head()

"""# Data Visualization - Matplotlib and Seaborn

## Line Chart
"""

import matplotlib.pyplot as plt

# Sample data
x = [1, 2, 3, 4, 5]
y = [10, 20, 25, 30, 40]

# Create a line plot
plt.plot(x, y, marker='o', linestyle='-', color='b')
plt.title('Basic Line Plot')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.grid(True)
plt.show()

"""## Bar Chart"""

# Categories and their corresponding values
categories = ['A', 'B', 'C', 'D']
values = [23, 45, 56, 78]

# Create a bar chart
plt.bar(categories, values, color='green')
plt.title('Bar Chart Example')
plt.xlabel('Categories')
plt.ylabel('Values')
plt.show()

"""## Histogram"""

import numpy as np

# Generate random data
data = np.random.randn(1000)

# Create a histogram
plt.hist(data, bins=30, color='purple', edgecolor='black')
plt.title('Histogram Example')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

"""## Scatter Plot"""

# Sample data
x = np.random.rand(50)
y = np.random.rand(50)

# Create a scatter plot
plt.scatter(x, y, color='red')
plt.title('Scatter Plot Example')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.show()

"""# Seaborn

## Distribution Plot - Identification of Normalized data
"""

import seaborn as sns

# Generate random data
data = np.random.randn(1000)

# Create a distribution plot
sns.histplot(data, bins=30, kde=True, color='skyblue')
plt.title('Distribution Plot')
plt.show()

"""## Box Plot - Outliers Identification"""

# Sample data
tips = sns.load_dataset('tips')

# Create a box plot
sns.boxplot(x='day', y='total_bill', data=tips, palette='Set2')
plt.title('Box Plot of Total Bill by Day')
plt.show()

"""## Violin Plot"""

# Create a violin plot
# sns.violinplot(x='day', y='total_bill', data=tips, palette='Pastel1')
# plt.title('Violin Plot of Total Bill by Day')
# plt.show()

"""## Heatmap"""

# Create a correlation matrix
corr = df_1_numeric.corr()

# Create a heatmap
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title('Heatmap of Correlation Matrix')
plt.show()

from sys import getsizeof

# List comprehension
list_comp = [i for i in range(10000)]
print(f"List size: {getsizeof(list_comp)} bytes")

# Generator expression
gen_expr = (i for i in range(10000))
print(f"Generator size: {getsizeof(gen_expr)} bytes")

